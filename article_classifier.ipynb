{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.Builder() \\\n",
    "     .appName(\"articles\") \\\n",
    "     .master(\"spark://spark-master:7077\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: hdfs://namenode:9000/data/articles.parquet;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o32.parquet.\n: org.apache.spark.sql.AnalysisException: Path does not exist: hdfs://namenode:9000/data/articles.parquet;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:641)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-861d7cbbe46d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://namenode:9000/data/articles.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: hdfs://namenode:9000/data/articles.parquet;'"
     ]
    }
   ],
   "source": [
    "df = ss.read.parquet(\"hdfs://namenode:9000/data/articles.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------+------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|                  id|               title|                sapo|                 url|        source|pega_cate_id|         title_token|          sapo_token|       content_token|           all_token|   label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------+------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|6.366651292038185E17|Anh phát hiện 39 ...|Ngày 23/10, cảnh ...|http://vnmedia.vn...|    vnmedia.vn|         102|Anh phát_hiện 39 ...|Ngày 23/10 , cảnh...|Theo cảnh_sát địa...|anh phát_hiện    ...|Thế giới|\n",
      "|6.368640877043630...|Phát hiện két sắt...|Theo TASS ngày 23...|http://congan.com...| congan.com.vn|         102|Phát_hiện két sắt...|( CAO ) Theo TASS...|Theo điều_tra ban...|phát_hiện két sắt...|Thế giới|\n",
      "|6.370351636924579...|Máy bay rơi ở Mex...|Theo Sputnik ngày...|http://congan.com...| congan.com.vn|         102|Máy_bay rơi ở Mex...|( CAO ) Theo Sput...|Chiếc máy_bay đan...|máy_bay rơi ở mex...|Thế giới|\n",
      "| 6.37236419836928E17|Hình ảnh Đệ nhất ...|- Giữa lúc tin đồ...|http://vnmedia.vn...|    vnmedia.vn|         102|Hình_ảnh Đệ nhất ...|- Giữa lúc tin_đồ...|HÌnh_ảnh Đệ nhất ...|hình_ảnh đệ nhất ...|Thế giới|\n",
      "|6.380422542719959E17|Thủ lĩnh cao nhất...|​(CAO) Hôm 27-10,...|http://congan.com...| congan.com.vn|         102|Thủ_lĩnh cao nhất...|​ ( CAO ) Hôm 27-...|Các nguồn_tin ở S...|thủ_lĩnh cao nhất...|Thế giới|\n",
      "|6.383146015204556...|Nhóm người di cư ...|Theo Daily Mail n...|http://congan.com...| congan.com.vn|         102|Nhóm người di_cư ...|( CAO ) Theo Dail...|Cảnh_sát Pháp sau...|nhóm người di_cư ...|Thế giới|\n",
      "|6.384845724814131...|Quân đội Iraq ban...|Ngày 28/10, quân ...|http://baotintuc....|  baotintuc.vn|         102|Quân_đội Iraq ban...|Ngày 28/10 , quân...|Người biểu_tình t...|quân_đội iraq ban...|Thế giới|\n",
      "|6.385290400436305...|Chân dung \"Chị Bì...|Được gọi với cái ...|http://afamily.vn...|       AFamily|         102|Chân_dung \" Chị_B...|Được gọi với cái ...|39 thi_thể được p...|chân_dung   chị_b...|Thế giới|\n",
      "|6.385365374207508...|Cuộc sống khổ cực...|Khi Li Hua nộp 14...|http://danviet.vn...|    danviet.vn|         102|Cuộc_sống khổ_cực...|Khi Li_Hua nộp 14...|Trong số các nạn_...|cuộc_sống khổ_cực...|Thế giới|\n",
      "|6.385404403925893...|Tài xế container ...|Người lái xe cont...|https://vtc.vn/ta...|        vtc.vn|         102|Tài_xế container ...|( VTC News ) - Ng...|( VTC News ) - Ng...|tài_xế container ...|Thế giới|\n",
      "|6.385850645113978...|Một góc nhìn về t...|Luật sư Hoàng Duy...|https://nhandan.c...|nhandan.com.vn|         102|Một góc nhìn về t...|Luật_sư Hoàng_Duy...|Trong bài viết gử...|một góc nhìn về t...|Thế giới|\n",
      "|6.385851275895357...|Đảng cầm quyền ở ...|Roi-tơ dẫn thông ...|https://nhandan.c...|nhandan.com.vn|         102|Đảng cầm_quyền ở ...|Roi - tơ dẫn thôn...|Theo CNE , Tổng_t...|đảng cầm_quyền ở ...|Thế giới|\n",
      "|6.385851275937300...|  Cơ hội để thay đổi|Đợt biểu tình kéo...|https://nhandan.c...|nhandan.com.vn|         102|  Cơ_hội để thay_đổi|Đợt biểu_tình kéo...|Khởi_phát từ thủ_...|cơ_hội để thay_đổ...|Thế giới|\n",
      "|6.385851487288279E17|Cô-lôm-bi-a: Rơi ...|Không quân Cô-lôm...|https://nhandan.c...|nhandan.com.vn|         102|Cô - lôm - bi - a...|Không_quân Cô - l...|Trong thông_cáo ,...|cô   lôm   bi   a...|Thế giới|\n",
      "|6.385851487288279E17|Vì một châu lục k...|Những vụ tiến côn...|https://nhandan.c...|nhandan.com.vn|         102|Vì một châu_lục k...|Những vụ tiến_côn...|Trong bối_cảnh xu...|vì một châu_lục k...|Thế giới|\n",
      "|6.386141457978736...|[Video] Cháy rừng...|Cháy rừng tại ban...|https://www.vietn...|vietnamplus.vn|         102|[ Video ] Cháy rừ...|Cháy rừng tại ban...|Ngày 27/10 , cháy...|  video   cháy rừ...|Thế giới|\n",
      "|6.386148180357529...|Khung cảnh tan ho...|Cuộc săn lùng kẻ ...|https://www.vietn...|vietnamplus.vn|         102|Khung_cảnh tan_ho...|Cuộc săn_lùng kẻ ...|Cuộc săn_lùng kẻ ...|khung_cảnh tan_ho...|Thế giới|\n",
      "|6.386167924372521E17|[Video] Biểu tình...|Các cuộc biểu tìn...|https://www.vietn...|vietnamplus.vn|         102|[ Video ] Biểu_tì...|Các cuộc biểu_tìn...|Các cuộc biểu_tìn...|  video   biểu_tì...|Thế giới|\n",
      "|6.386198699574804...|Tổng thống Chile ...|Tổng thống Chile ...|https://www.vietn...|vietnamplus.vn|         102|Tổng_thống Chile_...|Tổng_thống Chile_...|Tổng_thống Chile_...|tổng_thống chile_...|Thế giới|\n",
      "|6.386205023830261...|Mượn tay người Ku...|Giới phân tích ch...|https://vtc.vn/mu...|        vtc.vn|         102|Mượn tay người Ku...|( VTC News ) - Gi...|( VTC News ) - Gi...|mượn tay người ku...|Thế giới|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------+------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.drop('id', \n",
    "                'title', \n",
    "                'sapo',\n",
    "                'url',\n",
    "                'source',\n",
    "                'title_token',\n",
    "                'sapo_token',\n",
    "                'content_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------+\n",
      "|pega_cate_id|           all_token|   label|\n",
      "+------------+--------------------+--------+\n",
      "|         102|anh phát_hiện    ...|Thế giới|\n",
      "|         102|phát_hiện két sắt...|Thế giới|\n",
      "|         102|máy_bay rơi ở mex...|Thế giới|\n",
      "|         102|hình_ảnh đệ nhất ...|Thế giới|\n",
      "|         102|thủ_lĩnh cao nhất...|Thế giới|\n",
      "+------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize text in all_token columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "tkn = Tokenizer().setInputCol(\"all_token\").setOutputCol(\"content_tokenized\")\n",
    "train_df = tkn.transform(df_new)\n",
    "# train_df = tokenized.drop('title_token', 'sapo_token', 'content_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------+--------------------+\n",
      "|pega_cate_id|           all_token|   label|   content_tokenized|\n",
      "+------------+--------------------+--------+--------------------+\n",
      "|         102|anh phát_hiện    ...|Thế giới|[anh, phát_hiện, ...|\n",
      "|         102|phát_hiện két sắt...|Thế giới|[phát_hiện, két, ...|\n",
      "|         102|máy_bay rơi ở mex...|Thế giới|[máy_bay, rơi, ở,...|\n",
      "|         102|hình_ảnh đệ nhất ...|Thế giới|[hình_ảnh, đệ, nh...|\n",
      "|         102|thủ_lĩnh cao nhất...|Thế giới|[thủ_lĩnh, cao, n...|\n",
      "+------------+--------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"label_id\")\n",
    "hashingTF = HashingTF(inputCol=\"content_tokenized\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[hashingTF, idf, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(train_df)\n",
    "dataset = pipelineFit.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.withColumnRenamed(\"label\", \"label_name\")\n",
    "dataset = dataset.withColumnRenamed(\"label_id\", \"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+--------------------+--------------------+--------------------+-----+\n",
      "|pega_cate_id|           all_token|label_name|   content_tokenized|         rawFeatures|            features|label|\n",
      "+------------+--------------------+----------+--------------------+--------------------+--------------------+-----+\n",
      "|         102|anh phát_hiện    ...|  Thế giới|[anh, phát_hiện, ...|(10000,[44,277,57...|(10000,[44,277,57...|  1.0|\n",
      "|         102|phát_hiện két sắt...|  Thế giới|[phát_hiện, két, ...|(10000,[54,63,250...|(10000,[54,63,250...|  1.0|\n",
      "|         102|máy_bay rơi ở mex...|  Thế giới|[máy_bay, rơi, ở,...|(10000,[63,378,49...|(10000,[63,378,49...|  1.0|\n",
      "|         102|hình_ảnh đệ nhất ...|  Thế giới|[hình_ảnh, đệ, nh...|(10000,[37,43,52,...|(10000,[37,43,52,...|  1.0|\n",
      "|         102|thủ_lĩnh cao nhất...|  Thế giới|[thủ_lĩnh, cao, n...|(10000,[63,70,133...|(10000,[63,70,133...|  1.0|\n",
      "+------------+--------------------+----------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chia dữ liệu thành 2 tập train và test với tỷ lệ 80-20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = dataset.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', \n",
    "                                          metricName='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Huấn luyện mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(df_train)\n",
    "pred = lrModel.transform(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Đánh giá mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.821733459805549"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thử nghiệm Spark Streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = ss.read.parquet(\"hdfs://namenode:9000/data/Dstream/f1.parquet\")\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- title_token: string (nullable = true)\n",
      " |-- sapo_token: string (nullable = true)\n",
      " |-- content_token: string (nullable = true)\n",
      " |-- tag_token: double (nullable = true)\n",
      " |-- title_postag: string (nullable = true)\n",
      " |-- sapo_postag: string (nullable = true)\n",
      " |-- content_postag: string (nullable = true)\n",
      " |-- tag_postag: double (nullable = true)\n",
      " |-- title_ner: string (nullable = true)\n",
      " |-- sapo_ner: string (nullable = true)\n",
      " |-- content_ner: string (nullable = true)\n",
      " |-- tag_ner: double (nullable = true)\n",
      " |-- update_time: string (nullable = true)\n",
      " |-- source_tracking: string (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-------+-------------------+---------------+-----------------+\n",
      "|                id|         title_token|          sapo_token|       content_token|tag_token|        title_postag|         sapo_postag|      content_postag|tag_postag|           title_ner|            sapo_ner|         content_ner|tag_ner|        update_time|source_tracking|__index_level_0__|\n",
      "+------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-------+-------------------+---------------+-----------------+\n",
      "|783112665224601602|Huyền My hội_ngộ ...|Show thời_trang m...|Tối_ngày 29/11 ở ...|     null|Huyền/Np My/Np hộ...|Show/Nb thời_tran...|Tối_ngày/N 29/11/...|      null|Huyền_My/PER Hồng...|Huyền_My/PER Hồng...|Hà_Nội/LOC Fashio...|   null|2020-12-01 00:00:01|    newsdb.news|                0|\n",
      "|783116112820854786|Chủ_tịch Quốc_hội...|Tối 30-11 , tại Q...|Uỷ_viên Bộ_Chính_...|     null|Chủ_tịch/N Quốc_h...|Tối/N 30-11/M ,/C...|Uỷ_viên/N Bộ_Chín...|      null|Quốc_hội/ORG Danh...|Quảng_trường_Hồ_C...|Bộ_Chính_trị/ORG ...|   null|2020-12-01 00:00:01|    newsdb.news|                1|\n",
      "|783118176766550016|Nhìn lại thành_tự...|T1 có những người...|Với việc có cho m...|     null|Nhìn/V lại/R thàn...|T1/Ny có/V những/...|Với/E việc/N có/V...|      null|  Canna/PER Zeus/PER|              T1/PRO|Zeus/PER Canna/PE...|   null|2020-12-01 00:00:01|    newsdb.news|                2|\n",
      "|783118586843652109|Triệt_phá ổ nhóm ...|Ngày 30/11 , Phòn...|Ngày 30/11 , Phòn...|     null|Triệt_phá/V ổ/N n...|Ngày/N 30/11/M ,/...|Ngày/N 30/11/M ,/...|      null|                null|Phòng_Cảnh_sát_Hì...|Phòng_Cảnh_sát_Hì...|   null|2020-12-01 00:00:01|    newsdb.news|                3|\n",
      "+------------------+--------------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-------+-------------------+---------------+-----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = ss.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    ".parquet(\"hdfs://namenode:9000/data/Dstream/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"content_token\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"test\")\\\n",
    ".format(\"memory\").outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activityQuery.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_records:  382\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_recordscords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0264e6e5e1f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcount_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n_records: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new_records: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_recordscords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#     df.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_recordscords' is not defined"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "count_records = 0\n",
    "for x in range(5):\n",
    "    \"\"\"\n",
    "    TODO: xử lý API \n",
    "    \"\"\"\n",
    "    n_records = ss.sql(\"SELECT content_token FROM test \").count()\n",
    "    new_records = n_records - count_records\n",
    "#     df = ss.sql(\"Select content_token From test Limit {} OFFSET {}\".format(new_records, n_records - new_records))\n",
    "    count_records = n_records\n",
    "    print(\"n_records: \", n_records)\n",
    "    print('new_records: ', new_recordscords)\n",
    "#     df.show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
